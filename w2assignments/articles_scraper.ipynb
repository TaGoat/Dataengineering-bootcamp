{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from selenium import webdriver\n",
                "from selenium.webdriver.chrome.service import Service\n",
                "from selenium.webdriver.chrome.options import Options\n",
                "from webdriver_manager.chrome import ChromeDriverManager\n",
                "from bs4 import BeautifulSoup\n",
                "import time"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_driver():\n",
                "    options = Options()\n",
                "    options.add_argument('--headless')\n",
                "    options.add_argument('--disable-gpu')\n",
                "    options.add_argument('--no-sandbox')\n",
                "    options.add_argument('--disable-dev-shm-usage')\n",
                "    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
                "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
                "    return driver"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "def scrape_article_content(driver, url):\n",
                "    try:\n",
                "        driver.get(url)\n",
                "        time.sleep(3)\n",
                "        if '404' in driver.title.lower() or 'page not found' in driver.title.lower():\n",
                "            return None\n",
                "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
                "        article = soup.find('article')\n",
                "        if article:\n",
                "            paragraphs = article.find_all('p')\n",
                "            content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
                "        else:\n",
                "            paragraphs = soup.find_all('p')\n",
                "            content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
                "        return content if content else 'No content found'\n",
                "    except Exception as e:\n",
                "        return f'Error: {str(e)}'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def scrape_articles_to_csv(input_csv, output_csv, limit=10, delay=2):\n",
                "    df = pd.read_csv(input_csv).head(limit)\n",
                "    results = []\n",
                "    total = len(df)\n",
                "    driver = create_driver()\n",
                "\n",
                "    try:\n",
                "        for idx, row in df.iterrows():\n",
                "            article_id = row['id']\n",
                "            title = row['title']\n",
                "            url = row['url']\n",
                "\n",
                "            print(f'Scraping {idx + 1}/{total}: {title[:60]}...')\n",
                "            content = scrape_article_content(driver, url)\n",
                "            if content is None:\n",
                "                print(f'  ⚠ 404 - Skipped')\n",
                "                continue\n",
                "            results.append({\n",
                "                'Id': article_id,\n",
                "                'Title': title,\n",
                "                'Content': content\n",
                "            })\n",
                "            time.sleep(delay)\n",
                "    finally:\n",
                "        driver.quit()\n",
                "\n",
                "    result_df = pd.DataFrame(results)\n",
                "    result_df.to_csv(output_csv, index=False)\n",
                "    print(f'\\nDone! Saved {len(result_df)} articles to {output_csv}')\n",
                "    return result_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Scraping 1/20: Not All Rainbows and Sunshine: The Darker Side of ChatGPT...\n",
                        "Scraping 2/20: Ethics in AI: Potential Root Causes for Biased Algorithms...\n",
                        "Scraping 3/20: Python Tuple, The Whole Truth and Only the Truth: Let’s Dig ...\n",
                        "Scraping 4/20: Dates and Subqueries in SQL...\n",
                        "  ⚠ 404 - Skipped\n",
                        "Scraping 5/20: Temporal Differences with Python: First Sample-Based Reinfor...\n",
                        "  ⚠ 404 - Skipped\n",
                        "Scraping 6/20: Going Under the Hood of Character-Level RNNs: A NumPy-based ...\n",
                        "Scraping 7/20: ChatGPT isn’t all it seems, read this before you use it....\n",
                        "Scraping 8/20: 10 Subtle Strategies I Wish I Knew When I Had 23 Email Subsc...\n",
                        "Scraping 9/20: How To Start A Niche Site in Under 3 Hours (With Checklist)...\n",
                        "Scraping 10/20: Don’t Become a Full-Time Content Creator If You Have Low-Ris...\n",
                        "Scraping 11/20: Why Storytelling Is A Critical Skill For Startup Founders...\n",
                        "Scraping 12/20: Why My Side Hustle Stuck After 3 Years and What I Want to Do...\n",
                        "Scraping 13/20: Kill Your Perfectionism with an Abundance Mindset...\n",
                        "Scraping 14/20: <strong class=\"markup--strong markup--h3-strong\">How My MBA ...\n",
                        "Scraping 15/20: For this Writer, Self-Promotion isn’t Easy...\n",
                        "Scraping 16/20: How to Start Your Novel with Momentum to Finish It...\n",
                        "Scraping 17/20: <strong class=\"markup--strong markup--h3-strong\">Using Prope...\n",
                        "  ⚠ 404 - Skipped\n",
                        "Scraping 18/20: Coupon Collector’s Problem: A Probability Masterpiece...\n",
                        "Scraping 19/20: R for Data Analysis: How to Find the Perfect Cocomelon Video...\n",
                        "  ⚠ 404 - Skipped\n",
                        "Scraping 20/20: Sparkles aren’t good UX✨...\n",
                        "\n",
                        "Done! Saved 16 articles to articles_content.csv\n"
                    ]
                }
            ],
            "source": [
                "input_file = '../medium_data.csv'\n",
                "output_file = 'articles.csv'\n",
                "\n",
                "df_result = scrape_articles_to_csv(input_file, output_file, limit=20, delay=2)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
