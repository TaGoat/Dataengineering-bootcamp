Id,Title,Content
1,Not All Rainbows and Sunshine: The Darker Side of ChatGPT,"Publish AI, ML & data-science insights to a global community of data professionals. Part 1: The Risks and Ethical Issues Associated with Large Language Models If you haven’t heard about ChatGPT, you must be hiding under a very large rock. The viral chatbot, used for natural language processing tasks like text generation, is hitting the news everywhere. OpenAI, the company behind it, was recently in talks to get a valuation of $29 billion¹ and Microsoft may soon invest another $10 billion². ChatGPT is an autoregressive language model that uses deep learning to produce text. It has amazed users by its detailed answers across a variety of domains. Its answers are so convincing that it can be difficult to tell whether or not they were written by a human. Built on OpenAI’s GPT-3 family of large language models (LLMs), ChatGPT was launched on November 30, 2022. It is one of the largest LLMs and can write eloquent essays and poems, produce usable code, and generate charts and websites from text description, all with limited to no supervision. ChatGPT’s answers are so good, it is showing itself to be a potential rival to the ubiquitous Google search engine³. Large language models are … well…large. They are trained on enormous amounts of text data which can be on the order of petabytes and have billions of parameters. The resulting multi-layer neural networks are often several terabytes in size. The hype and media attention surrounding ChatGPT and other LLMs is understandable – they are indeed remarkable developments of human ingenuity, sometimes surprising the developers of these models with emergent behaviors. For example, GPT-3’s answers are improved by using the certain ‘magic’ phrases like ""Let’s think step by step"" at the beginning of a prompt⁴. These emergent behaviors point to their model’s incredible complexity combined with a current lack of explainability, have even made developers ponder whether the models are sentient⁵. With all the positive buzz and hype, there has been a smaller, forceful chorus of warnings from those within theResponsible AIcommunity. Notably in 2021, Timit Gebru, a prominent researcher working on responsible AI, published a paper⁶ that warned of the many ethical issues related to LLMs which led to her be fired from Google. These warnings span a wide range of issues⁷: lack of interpretability, plagiarism, privacy, bias, model robustness, and their environmental impact. Let’s dive a little into each of these topics. Deep learning models, and LLMs in particular, have become so large and opaque that even the model developers are often unable to understand why their models are making certain predictions. This lack of interpretability is a significant concern, especially in settings where users would like to know why and how a model generated a particular output. In a lighter vein, our CEO, Krishna Gade, used ChatGPT to create a poem⁸ on explainable AI in the style of John Keats, and, frankly, I think it turned out pretty well. Krishna rightfully pointed out that the transparency around how the model arrived at this output is lacking. For pieces of work produced by LLMs, the lack of transparency around which sources of data the output is drawing on means that the answers provided by ChatGPT are impossible to properly cite and therefore impossible for users to validate or trust its output⁹. This has led to bans of ChatGPT-created answers on forums like Stack Overflow¹⁰. Transparency and an understanding of how a model arrived at its output becomes especially important when using something like OpenAI’s Embedding Model¹¹, which inherently contains a layer of obscurity, or in other cases where models are used for high-stakes decisions. For example, if someone were to use ChatGPT to get first aid instructions, users need to know the response is reliable, accurate, and derived from trustworthy sources. While various post-hoc methods to explain a model’s choices exist, these explanations are often overlooked when a model is deployed. The ramifications of such a lack of transparency and trustworthiness are particularly troubling in the era of fake news and misinformation, where LLMs could be fine-tuned to spread misinformation and threaten political stability. While Open AI is working on various approaches to identify its model’s output and plans to embed cryptographic tags to watermark the outputs¹², these Responsible AI solutions can’t come fast enough and may be insufficient. This leads to issues around … Difficulty in tracing the origin of a perfectly crafted ChatGPT essay naturally leads to conversations on plagiarism. But is thisreallya problem? This author does not think so. Before the arrival of ChatGPT, students already had access to services that would write essays for them¹³, and there has always been a small percentage of students who are determined to cheat. But hand-wringing over ChatGPT’s ability to turn all of our children into mindless, plagiarizing cheats has been on the top of many educators’ minds and has led some school districts to ban the use of ChatGPT¹⁴. Conversations on the possibility of plagiarism detract from the larger and more important ethical issues related to LLMs. Given that there has been so much buzz on this topic, I’d be remiss to not mention it. Large language models are at risk for data privacy breaches if they are used to handle sensitive data. Training sets are drawn from a range of data, at times including personally identifiable information¹⁵ – names, email addresses¹⁶, phone numbers, addresses, medical information – and therefore, may be in the model’s output. While this is an issue with any model trained on sensitive data, given how large training sets are for LLMs, this problem could impact many people. As previously mentioned, these models are trained on huge corpuses of data. When data training sets are so large, they become very difficult to audit and are therefore inherently risky⁵. Thisdata contains societal and historical biases¹⁷ and thus any model trained on it is likely to reproduce these biases if safeguards are not put in place. Many popular language models were found to contain biases which can result in increases in the dissemination of prejudiced ideas and perpetuate harm against certain groups. GPT-3 has been shown to exhibit common gender stereotypes¹⁸, associating women with family and appearance and describing them as less powerful than male characters. Sadly, it also associates Muslims with violence¹⁹, where two-thirds of responses to a prompt containing the word ""Muslim"" contained references to violence. It is likely that even more biased associations exist and have yet to be uncovered. Notably, Microsoft’s chatbot quickly became a parrot of the worst internet trolls in 2016²⁰, spewing racist, sexist, and other abusive language. While ChatGPT has a filter to attempt to avoid the worst of this kind of language, it may not be foolproof. OpenAI pays for human labelers to flag the most abusive and disturbing pieces of data, but the company they contract with has faced criticism for only paying their workers $2 per hour and the workers report suffering from deep psychological harm²¹. Since LLMs come pre-trained and are subsequently fine tuned to specific tasks, they create a number of issues and security risks. Notably, LLMs lack the ability to provide uncertainty estimates²². Without knowing the degree of confidence (or uncertainty) of the model, it’s difficult for us to decide when to trust the model’s output and when to take it with a grain of salt²³. This affects their ability to perform well when fine-tuned to new tasks and to avoid overfitting. Interpretable uncertainty estimates have the potential to improve the robustness of model predictions. Model security is a looming issue due to an LLM’s parent model’s generality before the fine tuning step. Subsequently, a model may become a single point of failure and a prime target for attacks that will affect any applications derived from the model of origin. Additionally, with the lack of supervised training, LLMs can be vulnerable to data poisoning²⁵ which could lead to the injection of hateful speech to target a specific company, group or individual. LLM’s training corpuses are created by crawling the internet for a variety of language and subject sources, however they are only a reflection of the people who are most likely to have access and frequently use the internet. Therefore, AI-generated language is homogenized and often reflects the practices of the wealthiest communities and countries⁶. LLMs applied to languages not in the training data are more likely to fail and more research is needed on addressing issues around out-of-distribution data. A 2019 paper by Strubell and collaborators outlined the enormous carbon footprint of the training lifecycle of an LLM²⁴ ²⁶, where training a neural architecture search based model with 213 million parameters was estimated to produce more than five times the lifetime carbon emissions from the average car. Remembering that GPT-3 has 175billionparameters, and the next generation GPT-4 is rumored to have 100trillionparameters, this is an important aspect in a world that is facing the increasing horrors and devastation of a changing climate. Any new technology will bring advantages and disadvantages. I have given an overview of many of the issues related to LLMs, but I want to stress that I am also excited by the new possibilities and the promise these models hold for each of us. It is society’s responsibility to put in the proper safeguards and use this new tech wisely. Any model used on the public or let into the public domain needs to be monitored, explained, and regularly audited for model bias. In part 2, I will outline recommendations for AI/ML practitioners, enterprises, and government agencies on how to address some of the issues particular to LLMs. References: Written By  Share This Article Towards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program. A deep dive on the meaning of understanding and how it applies to LLMs Practical insights and analysis from experiments with MMLU-Pro On the impressive abilities of a new model - and what might happen if AI models became… Our weekly selection of must-read Editors’ Picks and original features Taking a look at some of our most memorable and popular stories from the past… Our weekly selection of must-read Editors’ Picks and original features Our weekly selection of must-read Editors’ Picks and original features Your home for data science and Al. The world’s leading publication for
				data science, data analytics, data engineering, machine learning, and
				artificial intelligence professionals. Is the AI and Data Job Market Dead? | Towards Data Science Detecting and Editing Visual Objects with Gemini | Towards Data Science How to Personalize Claude Code | Towards Data Science How to Run Claude Code for Free with Local and Cloud Models from Ollama | Towards Data Science"
2,Ethics in AI: Potential Root Causes for Biased Algorithms,"Publish AI, ML & data-science insights to a global community of data professionals. An alternative approach to understanding bias in data A s the number of data science applications increases, so does the potential for abuse. It is easy to condemn developers or analytics teams for the results of their algorithms, but are they the main culprits? The following article tries to discuss the problem from a different angle andconcludes that ethical abuse might be the real problem with data in our society. A better world won’t come about simply because we use data; data has its dark underside. ¹ Today, discussions of data science are often associated with significant opportunities in business and industry to deliver more accurate predictive and classification solutions and improve people’s lives in the health and environmental sectors.² ³ Perhaps as important as the benefits are the data ethics challenges that should be considered when evaluating new solutions and approaches in data analytics. As a specific branch of ethics, data ethics addresses moral issues related to data, algorithms, and the respective practices to formulate and support morally good solutions.⁴ Overall, there seems to be a fine line between the use and misuse of data. It is common for companies to collect data not only to increase their profits but also to provide a tailored and targeted experience to their customers.⁵ However, when companies start to externally exploit their data for purposes other than those for which it was collected, ethical questions arise. In addition to these privacy-related issues, another challenge is data analysis bias, which can arise in several ways. Examples include the creation of survey questions by people with a particular intent or framing, the selective collection of data from groups with a particular background, or the underlying bias of those from whom the data is drawn.⁶ The issues are not at all theoretical – they lead to real concerns and cause serious discussions as described in several case studies: Racial discrimination in crime predictionA problematic and well-known example is the prediction of crime. A 2016 machine learning paper by Shanghai Jiao Tong University investigated whether crime can be detected in people based on facial feature analysis.⁷ The result was that members of the population who look ""different"" from the average are more likely to be suspected of committing crimes. ⁸ Another example of biased data science approaches is the risk assessment algorithm used in the United States to predict the likelihood of re-offending (repeated criminal behavior) by arrested individuals. It turned out that the underlying algorithm tended to predict the likelihood of recidivism of black defendants twice as high as that of white defendants, while white defendants tended to be incorrectly classified as low-risk when in fact they were recidivists.⁹ Gender discrimination in recruitmentAnother area that is affected by biased algorithms is recruitment. In 2014, Amazon used machine learning-based software to evaluate and rank the CVs of potential candidates to find new top talent. In 2015, the company discovered that its new system was not evaluating applicants in a gender-neutral way, especially for software development jobs. Based on hiring over the last ten years, the system penalized all applications that included the word ""women"" in their CVs.¹⁰ Stereotypes in the social mediaGender and race bias can also be found in Facebook’s advertising. In their 2019 empirical study, Ali et al. discovered that there may be biased rules for delivering ads to specific users, based on the predicted relevance of those ads to different audiences. This relevance is often based on male/female stereotypes, classified and correlated through image analysis software that analyses digital advertising media.¹¹ How can this be prevented? These cases are just a few of many instances where algorithms are used in a biased way in today’s world. Many experts discuss and evaluate ways to avoid bias in data analysis, which is expressed in one of the ""5 Principles for Big Data Ethics"" published by Herdiana: ""Big Data should not institutionalize unfair biases such as racism or sexism."" ¹² While this principle is considered a general rule within data ethics, the examples given ""such as racism or sexism"" seem to be the most common occurrences. However, a more general description might be: ""Big data should not institutionalize unfair biases that cause discrimination"". The Canadian Human Rights Commission (CHRC) describes discrimination as an act or decision that treats a person or group poorly for reasons called grounds. These grounds are protected by the Canadian Human Rights Act and includerace, national or ethnic origin, color, religion, age, sex, sexual orientation, gender identity or expression, marital status, family status, disability, genetic characteristics, and a conviction for which a pardon has been granted or registration suspended. ¹³ In sociology, discrimination describes the belittling of others based on individual or group characteristics, practiced systemically and therefore constituting a gross violation of human rights.¹⁴ The European Convention on Human Rights (ECHR) also prohibits discrimination in all of the above areas in Article 14, adding to the CHRC listlanguage, political opinion,andpropertyas well.¹⁵ The European Economic and Social Committee (EESC) study entitled ""The ethics of Big Data: Balancing economic benefits and ethical questions of Big Data in the EU policy context"" describes algorithm bias as one of the critical ethical problems of Big Data. They define trust in algorithms as an ethical problem because most people think that machines are neutral by definition. In reality, this is not the case, and therefore the risk can be very high.¹⁶ But what are possible reasons for this lack of neutrality? It is hard to imagine that the best data scientists in the world have developed their algorithms with this goal in mind. Algorithms are not biased. Nevertheless, there exist many popular examples of skewed outcomes from algorithms while deeper assessments of the causes and drivers of the current outcome are often superficial. Moreover, the possible alternatives and outcomes in a hypothetical world without algorithms are missing. Therefore, the question of biased algorithms should not focus exclusively on the neutrality of algorithms, but also highlight the (missing) causes. The following analysis, therefore, aims to further explore issues related to bias and how today’s ethical pressure on developers could be understood as a collective responsibility. Nevertheless, it is crucial to underline the importance of eliminating discrimination in both technical and non-technical world, as the two are directly linked and influence each other: Technical decisions vs. human decisionsThere is no guarantee that humans make less biased decisions than algorithms. In addition to the idea that any dataset can be biased and lead to biased results in an algorithmic process, another key indicator of unfair predictions/classifications is the lack of information, in the form of incomplete data.²² It is difficult to guarantee that a given dataset possesses full visibility to explain a target behavior. Common reasons for the lack of data may be limited access, active exclusion of information, or lack of knowledge about the marginal impact of additional information. Another risk for discrimination does not only result from the technical implementation but can also be rooted in the existing structures of our society. The advantage of Big Data in combination with machine learning algorithms is the possibility to apply human rules and decisions to a large amount of information. If algorithms are considered biased, one might wonder how humans would have processed the analysis of given data. Looking back to social media advertising, personalized advertising aims to make individual suggestions based on one’s preferences. This implies that algorithms support individualism. However, algorithms have the opposite effect, as they process automated and standardized decision rules based on general rather than individual patterns. Ultimately, it can be assumed thatalgorithms support collectivismrather than individualism.²³ Our analysis has shown that the bias in algorithms is mainly a result of bias in today’s society. The latter supports the collection of biased datasets, which may contain (hidden) relationships, ultimately leading to biased results. Furthermore, we question whether human decision-making is less biased compared to algorithmic rules. Lastly, we have concluded that more complex algorithms are often too non-transparent. Mitigation strategyIn discussions about the responsibility for biased results of machine learning algorithms, the developer or the data science team is often blamed for discriminatory behavior. On one hand, this argument is overly simplistic, and the apportionment of the blame needs to be further differentiated. On the other, any developer working with machine learning algorithms should be aware of the sensitivity of their task and their respective responsibilities. Since algorithms support collectivism, it is important to investigate the fairness of these collective algorithms that make decisions or suggestions for individuals. Literature suggests other techniques to reduce bias, such as a system design that randomizes a small sample of queries or the use of propensity weighting techniques.²⁴ For bias in human-based content, there are several ideas for quantifying discrimination and assessing the fairness of algorithms.²⁵ Fairness as a shared responsibility: both in the technical and real worldIt is the responsibility of data teams to identify and minimize biased behavior in their data models. However, looking at the discriminatory factors examined in this article, it is a global responsibility to ensure that no one experiences such injustices, either through technical systems or human actions: Outlook: now what?It seems important to evaluate decisions or predictions not only based on the outcome itself but also based on the ethical considerations involved in working out the solution. We also need to rethink systems that involve biased behavior. However, there is no evidence that the alternative (human-based decision-making) leads to more equality in general. Consequently, we should not only focus on the problems with biaswithintechnology but rather broaden the discussion to include biasoutsidetechnology. Nevertheless,precisely because of this,unbiased systems should help improve our current social weaknessesand possibly lead tomore equality in the world to come.People could use AI to more easily detect discrimination in the non-technological world. Becausewherever we find bias in the data, there is also bias in society. Jonas Dieckmann – Medium I hope you find it interesting. Let me know your thoughts and feel free to connect on LinkedInhttps://www.linkedin.com/in/jonas-dieckmannand/or to follow me here on medium. How to get started with TensorFlow using Keras API and Google Colab Case Study: Applying a Data Science Process Model to a Real-World Scenario [1]: Loukides, M., Mason, H., & Patil, D. (2018). Ethics and Data Science. [2]: Floridi, L., & Taddeo, M. (2016). What is data ethics? Philosophical Transactions Of The Royal Society A: Mathematical, Physical, And Engineering Sciences, 374(2083), 20160360. doi: 10.1098/rsta.2016.0360 [4] Mittelstadt, B., & Floridi, L. (2015). The Ethics of Big Data: Current and Foreseeable Issues in Biomedical Contexts. Science And Engineering Ethics, 22(2), 303–341. DOI: 10.1007/s11948–015- 9652–2 [5] Hand, D. (2018). Aspects of Data Ethics in a Changing World: Where Are We Now? Big Data, 6(3), 176–190. DOI: 10.1089/big.2018.0083 [6] Broad, E., Smith, A., & Wells, P. (2017). Helping organizations navigate ethical concerns in their data practices. Open Data Inst. [7] Wu, Xiaolin & Zhang, Xi. (2016). Automated Inference on Criminality using Face Images. [8] Sullivan, B. (2016). A New Program Judges If You’re a Criminal From Your Facial Features. Retrieved 10 March 2021, fromhttps://www.vice.com/en/article/d7ykmw/new-programdecides-criminality-from-facial-features [9] Angwin, J., Larson, J., Mattu, S., & Kirchner, L. (2016). Machine Bias. Retrieved 28 February 2021, fromhttps://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing [10] Dastin, J. (2018). Amazon scraps secret AI recruiting tool that showed bias against women. Retrieved 28 February 2021, fromhttps://www.reuters.com/article/us-amazon-com-jobsautomation-insight-idUSKCN1MK08G [11] Ali, M., Sapiezynski, P., Bogen, M., Korolova, A., Mislove, A., & Rieke, A. (2019). Discrimination through Optimization. Proceedings Of The ACM On Human-Computer Interaction, 3(CSCW), 1–30. DOI: 10.1145/3359301 [12] Herdiana (2013), ""5 Principles for Big Data Ethics,"" Journal of Chemical Information and Modeling. Retrieved 10 March 2021, from:https://aiforceos.com/2018/06/16/big-data-ethics [13] Canadian Human Rights Commission. What is Discrimination. Retrieved 28 February 2021, fromhttps://www.chrc-ccdp.gc.ca/eng/content/what-discrimination [14] Hartfiel, G., & Hillmann, K. (1994). Dictionary of Sociology. Stuttgart: Kröner [15] Council of Europe (1950). European Convention for the Protection of Human Rights and Fundamental Freedoms. Retrieved 10 March 2021, fromwww.refworld.org/docid/3ae6b3b04 [16] European Economic and Social Committee. (2017). The ethics of Big Data. [17] Eslami, M., Vaccaro, K., Karahalios, K. and Hamilton, K. (2017) ‘Be Careful; Things Can Be Worse than They Appear’: Understanding Biased Algorithms and Users’ Behavior Around Them in Rating Platforms. Proceedings of the International AAAI Conference on Web and Social Media, 11(1). [18] Kim, P. (2017) Auditing Algorithms for Discrimination, 166 U. Pa. L. Rev. Retrieved 10 March 2021, from scholarship.law.upenn.edu/penn_law_review_online/vol166/iss1/10 [19] Mittelstadt, B., Allo, P., Taddeo, M., Wachter, S., & Floridi, L. (2016). The ethics of algorithms: Mapping the debate. Big Data & Society, 3(2), 205395171667967. DOI: 10.1177/2053951716679679 [20] Lehr, D., Ohm, P. (2017). Playing with the Data: What Legal Scholars Should Learn About Machine Learning, UCDL Review 51: 653, 671. [21] Calders, T., & Žliobaitė, I. (2013). Why Unbiased Computational Processes Can Lead to Discriminative Decision Procedures. Studies In Applied Philosophy, Epistemology, And Rational Ethics, 43–57. DOI: 10.1007/978–3–642–30487–3_3 [22] Rosen, R. (2019). Missing Data and Imputation. Retrieved 10 March 2021, fromhttps://towardsdatascience.com/missing-data-and-imputation-89e9889268c8 [23] Saake. I., Nassehi, A. (2004). Die Kulturalisierung der Ethik. Eine zeitdiagnostische Anwendung des Luhmannschen Kulturbegriffs. Frankfurt/M: 102, 135 [24] Ridgeway, G., Kovalchik, S., Griffin, B., & Kabeto, M. (2015). Propensity Score Analysis with Survey Weighted Data. Journal Of Causal Inference, 3(2), 237–249. doi: 10.1515/jci-2014–0039 [25] Floridi, L. (2016). Faultless responsibility: on the nature and allocation of moral responsibility for distributed moral actions. Philosophical Transactions Of The Royal Society A: Mathematical, Physical, And Engineering Sciences, 374(2083), 20160112. doi: 10.1098/rsta.2016.0112 Written By  Share This Article Towards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program. Step-by-step code guide to building a Convolutional Neural Network Here’s how to use Autoencoders to detect signals with anomalies in a few lines of… Solving the resource constrained project scheduling problem (RCPSP) with D-Wave’s hybrid constrained quadratic model (CQM) An illustrated guide on essential machine learning concepts Derivation and practical examples of this powerful concept Columns on TDS are carefully curated collections of posts on a particular idea or category… With demos, our new solution, and a video Your home for data science and Al. The world’s leading publication for
				data science, data analytics, data engineering, machine learning, and
				artificial intelligence professionals. Is the AI and Data Job Market Dead? | Towards Data Science Detecting and Editing Visual Objects with Gemini | Towards Data Science How to Personalize Claude Code | Towards Data Science How to Run Claude Code for Free with Local and Cloud Models from Ollama | Towards Data Science All articles on Towards Data Science are available to registered users. Create a free account today to access this article and thousands more."
3,"Python Tuple, The Whole Truth and Only the Truth: Let’s Dig Deep","Publish AI, ML & data-science insights to a global community of data professionals. Learn the intricacies of tuples . In the previous article, we discussed the basics of tuples: Python Tuple, the Whole Truth, and Only the Truth: Hello, Tuple! I showed you what a tuple is, what methods it offers, and most importantly, we discussed tuples immutability. But there is far more into tuples than that, and this articles offers continuation of the previous one. You will learn here the following aspects of the tuple type: Likely the most important intricacy of the tuple is its immutability. But since it creates the essence of this type, even beginners should know how this immutability works and what it means in both theory and practice; thus, we’ve discussed it in the above-mentioned previous article. Here, we will discuss other important intricacies of tuples. This will be fun! A theorist would likely scream at me that there is only one immutability of tuples, the one we discussed in the previous article. Well, that’s true, but… but Python itself makes a distinction between two different types of immutability! And Pythonmustmake this distinction. This is because only a truly immutable object is hashable. In the below code, you will see that the first tuple is hashable but the second one is not: Whether or not an object is hashable affects various things – and this is why Python differentiates hashable and non-hashable tuples; the former are what I call truly immutable tuples. I will show you how Python treats both in how copying of tuples works, and in using tuples as dictionary keys. First, let’s see how it works in tuple copying. For this. let us create a fully immutable tuple and copy it using all the available methods: Sinceais a fully immutable tuple, the original tuple (a) and all its copies should point to the very same object: As expected – and as should be the case for a truly immutable type – all these names point to the very same object; theirids are the same. This is what I call true or full immutability. Now let’s do the same with a tuple of the second type; that is, a tuple with one or more mutable elements: The copies frombtoeare shallow, so they will refer to the same object as the original name: This is why we have deep copying. A deep copy should cover all the objects, including those nested inside. And since we have a mutable object inside theatuple, then unlike before, the deep copyfthis time willnotpoint to the same object: The first element (at index0) of the tuple is[1], so it’s mutable. When we created the shallow copies ofa, the first elements of the tuplesatoepointed to the same list: but creating a deep copy meant creating a new list: Now let’s see how these two types of immutability work differ in terms of using tuples as dictionary keys: So, if you want to use a tuple as a dictionary key, it must be hashable – so it must be truly immutable. So, if anyone tells you that there is only one type of immutability of Python tuples, you will know that’s not entirely true – as there are two sorts of tuples in terms of immutability: Failing to distinguish them would disable you to understand how copying of tuples works. Type hints have been becoming more and more important in Python. Some say that there’s no modern Python code without type hints. As I wrote what I think in another article, I will not repeat myself here. If you’re interested, feel invited to read it: Python’s Type Hinting: Friend, Foe, or Just a Headache? Here, let’s shortly discuss how to deal with type hints for tuples. I will show the modern version to type hinting tuples, meaning Python 3.11. As type hinting has been dynamically changing, however, be aware that not everything worked the same way in older Python versions. As of Python 3.9, things got simpler, as you can use built-intupletype with fields indicated in square brackets[]. Below are several examples of what you can do. tuple[int, ...],tuple[str, ...]and the like
This means the object is a tuple ofint/str/ and the like elements, of any length. The ellipsis,..., informs that the tuple can have any length; there is no way to fix it. tuple[int | float, ...]Like above, but the tuple may contain bothintandfloatitems. tuple[int, int]Unlike above, this tuple is a record of two items, both being integers. tuple[str, int|float]Again, a record of two items, the first being a string and the second an integer or a floating-point number. tuple[str, str, tuple[int, float]]A record with three items, the first two being strings and the third one being a two-element tuple of an integer and a floating-point number. tuple[Population, Area, Coordinates]This is a specific record, one that contains three elements of specific types. These types,Population,Area,Coordinatesare either named tuples or data types defined earlier, or type aliases. As I explained in the above-mentioned article, using such type aliases can be much more readable than using the built-in types such asint,float, and the like. These were just several examples, but I hope they will help you see what you can do with type hinting for tuples. I have only mentionednamed tuples, as I will discuss this type in another section below. Do remember, however, that named tuples are of much help also in the context of type hinting, as thanks to a named tuple you can get a custom type alias that is also a data container – a powerful combination. You can inherit fromlist, though sometimes it’s better to inherit fromcollections.UserList. So, maybe we can do the same with the tuple? Can we inherit from thetupleclass? Basically, forget the idea of creating a tuple-like general type. Thetupledoes not have its own.__init__()method, so you cannot do what you can when inheriting from the list, that is, you cannot callsuper().__init__(). And without that, you’re left with almost nothing as thetupleclass inheritsobject.__init__()instead. Nonetheless, this does not mean you cannot inherit fromtupleat all. You can, but not to create a general type, but a specific one. Do you remember theCityclass? We can do something similar with a tuple – but be aware that this will be no fun. We have a tuple-likeCityclass: This class takesexactlyfour arguments, not fewer and not more: Note that in the current version, we can use argument names but do not have to, as they are positional: But we cannot access the values by names: We can change that in two ways. One is by using a named tuple from thecollectionsortypingmodule; we will discuss them soon. But we can achieve the same effect using ourCityclass, thank to theoperatormodule: And now we can accesslatandlongattributes by name: However, since we did the above only forlatandlong, we will not be able to accesspopulationandareaby name: We can of course change that: I’ve never done anything like this, however. If you want to have such a functionality, you should definitely use a named tuple instead. To benchmark various operations using tuples and, for comparison, lists, I used the script presented in Appendix close to the end of the article. You will also find there the results of running the code. I present the code not only for the record, but to enable you to extend the experiment. Overall, the list wasalwaysfaster, irrespective of its size and the operation being performed. I’ve often heard that one of the reasons behind creating tuples was their smaller memory consumption. Our little experiment is far from confirming this idea. While indeed sometimes tuples used a little less memory, usually they used a little more. Hence I conducted the experiment for really long lists and tuples, of 5 mln and 10 mln integer items. And again, lists usually consumed less memory… So, where is this small memory consumption of tuples? Perhaps it’s related to how much disk space a tuple and the corresponding list take? Let’s check: Only in the case of small tuples and their corresponding lists, the difference in memory use is noticeable – like, for instance,152against168. But I think you’ll agree with me that400_000_032is not really that much smaller than400_000_048, won’t you? There’s one more thing I observed in my past experiments (code not presented). Tuple literals are treated in an exceptional way by the Python compiler, as it keeps them in the static memory – so they are created at compile time. Neither lists nor tuples created in any other way can be kept in static memory – they always use dynamic memory, which means they are created at run time. This topic is complex enough to deserve a separate article, so let’s stop here. I’ll leave you here with these benchmarks. If you want to extend them, go ahead. If you learn something new and unexpected, please share this in the comments. What I have learned is that tuples should almost never be used only because of their performance. But indeed, tuples can be an interesting choice if we need a simple type to store really small records, like consisting of two or three elements. If field names would help, however, and for more fields, I’d rather use something else, a named tuple being one of the choices and adataclasses.dataclassanother. InFluent Python, L. Ramalho mentions two advantages of a tuple over a list: clarity and performance. Honestly, I cannot find any other advantage, but these two can be enough. So, let’s discuss them one by one and decide if they indeed make tuples better than lists, at least in some aspects. As L. Ramalho writes, when you’re using a tuple, you know its length will never change – and this increases the clarity of code. We have already discussed what can happen with a tuple’s length. Indeed, clarity due to immutability is a great thing, and we do know that the length of any tuple willneverchange, but… As L. Ramalho warns himself, a tuple with mutable items can be a source of bugs that are difficult to find. Do you remember what I mentioned above in relation to in-place operations? On the one hand, we may be sure that a tuple, sayx, will never change its length. It’s a valuable piece of information in terms of clarity, I agree. However, when we perform in-place operation(s) onx, this tuple willstopbeing the same tuple, even though it will remain a tuple namedx– but, let me repeat, a different tuple namedx. Thus, we should revise the above clarity advantage as follows: Or: Sounds a little crazy? I fully agree: this is crazy. For me, this is no clarity; this is the opposite of clarity. Does anyone think that way? Imagine you have a function in which you define a tuplex. You then perform in-place concatenation, e.g.,x += y, so it looks as thoughyremained untouched butxchanged. We know it’s not true – as this originalxdoes not exist anymore and we have a brand newx– but this is what itlooks like, especially because we still have a tuplexwhose first elements are the very same ones that constituted the originalxtuple. Sure, I know all this makes sense from a Python point of view. But when I’m coding, I do not want my thoughts to be occupied that way. For code to be clear, I prefer it to be clear without the necessity of making such assumptions. And this is the very reason why for me tuples do not mean clarity; they mean less clarity than I see in lists. This is not all in the context of the tuple’s clarity. In terms of code, there is one thing I particularly like in lists but do not like in tuples. Square brackets[]used to create lists allow them to stand out in the code, as there is no other container that would use square brackets. Look at dictionaries: they use curly brackets{}, and these can be used by sets, too. Tuples use round parentheses(), and these are used not only in generator expressions but in many various places in code, as Python code uses round parentheses for many different purposes. Therefore, I like how lists stand out in code – and I don’t like how tuples donot. L. Ramalho, writes that a tuple uses less memory than the corresponding list, and Python can do the same optimizations for both. We have already analyzed memory performance as we know that it’s not always the case – and that the disk memory a tuple uses is indeed smaller than that the corresponding list uses, but the difference can be negligible. This knowledge, combined with the better performance of lists in terms of execution time, makes me think that performance doesnotmake tuples a better choice. In terms of performance in terms of execution time, lists are better. In terms of memory usage, tuples can be better indeed – but these days, with the modern computers, differences are really small. Besides, when I need a truly memory-efficient container to collect a lot of data, I’d choose neither list nor tuple – but a generator. In addition to these two aspects, there is a third one that’s worth consideration, one we have already mentioned – you cannot use lists as keys in dictionaries, but you can use tuples. Or rather, you can use truly immutable (that is, hashable) tuples. The reason is the former’s mutability and the latter’s immutability. Unlike the previous two, this advantage can be significant in particular situations, even if rather rare ones. If you hope to learn from this section that there are tuple comprehensions in Python, or if you hope to learn something amazing that will blew minds of your fellow Pythonistas – I am so sorry! I did not want to create false hopes. No tuple comprehensions today; no mind-blowing syntax. You may remember that in my article on Python comprehensions, I did not mention tuple comprehensions: A Guide to Python Comprehensions This is because there are no tuple comprehensions. But as I do not want to leave you with nothing, I do have a consolation gift for you. I’ll show you some substitutes for tuple comprehensions. First of all, do remember that a generator expression isnota tuple comprehension. I think many Python beginners make a mistake of confusing the two. I specifically remember seeing my first generator expression after learning list comprehensions. My first thought was, ""Yup, here it is. A tuple comprehension."" I quickly learned that while the first from these two was indeed a list comprehension, the second wasnota tuple comprehension: I spent some time – if notwastedit – only to learn that there are list comprehensions, and set comprehensions, and dict comprehensions, and generator expressions – but no tuple comprehensions. Don’t repeat my mistake. Don’t spend hours on looking for tuple comprehensions. They don’t exist in Python. But here it is, my consolation gift for you – two substitutes for tuple comprehensions. Substitute 1:tuple()+genexp Have you noticed that you do not need to create a list comprehension first and the tuple then? Indeed, here we create a generator expression and use thetuple()class to it. This, of course, gives us a tuple. Substitute 2:genexp+ generator unpacking A nice hack, isn’t it? It usesthe extended unpacking of iterables, which returns a tuple. You can use it for any iterable, and since a generator is one, it works! Let’s check if it works also for a list: You can do the same without assigning tox: It will work for any iterable – but don’t forget the comma at the line end; without it, the trick will not work: Let’s check for sets: It works! And note that generally, unpacking provides a tuple. This is why the extended iterable unpacking looks a little bit like a tuple comprehension. Although it does look like a nice little hack, it’s not: it’s one of the tools that Python offers, although it’s an edge case indeed. But I wouldnotusesubstitute 2. I’d definitely go forsubstitute 1, which usestuple(). Most of us love tricks like the second substitute, but they are seldom clear – andsubstitute 2, unlikesubstitute 1, is far from being clear. Nevertheless, any Pythonista will see what’s going on insubstitute 1, even if they do not see that there’s a generator expression hidden in an in-between step. Tuples are unnamed – but this does not mean there are no named tuples in Python. On the contrary, there are – and, unsurprisingly, they are called…named tuples. You have two possibilities to use named tuples:collections.namedtupleandtyping.NamedTuple. Named tuples are what their named suggest: tuples whose elements (called fields) have names. You can see the former in action in Appendix, in the benchmarking script. Personally, I consider them extremely helpful in many various situations. They do not offer any improvement in performance; they can even decrease it. But when it comes to clarity, they can be much clearer, both to the developer and to the code’s user. Thus, although I often go for a regular tuple, sometimes I decide to choose a named tuple – and this is exactly because of its clarity. Named tuples offer so rich possibilities that they deserve their own article. Therefore, that’s all I’m going to tell you about them here – but I plan to write an article dedicated to this powerful type. This article, along withthe previous one, aimed to provide you with deep information about tuples, their use cases, pros and cons, and intricacies. Although tuples are used quite often, they are not that well known among developers, particularly those with shorter experience in Python development. That’s why I wanted to collect rich information about this interesting type in one place – and I hope you’ve learned something from reading it, maybe even as much as I learned myself from writing it. To be honest, when starting to write about tuples, I thought that I’d find more advantages of them. I’ve been using them from the first day I started using Python. Although I used lists far more often, I somehow liked tuples, even though I did not know too much about them – so some of the information I included in this article was new to me. After writing this article, however, I am not that great a fan of tuples anymore. I still consider them a valuable type for small records, though quite often their extension – named tuples – or data classes seem a better approach. What’s more, tuples do not seem to be too effective. They are slower than lists, and use only a little less memory. So, why should I use them? Maybe because of their immutability? Maybe. If you like functional programming, which is based upon the concept of immutability, you will definitely prefer tuples over lists. I used this argument not once and not twice to convince myself that I should prefer a tuple over a list in this or another situation. But the immutability that the tuple offers is, as we discussed, not thatclear.Imaginex, a tuple of items of immutable types. We know this tuple is trule immutable, right? If so, I do not like the following code, which is fully correct in Python: I know this is correct Python, and I know this is even Pythonic code – but I don’t like it. I don’t like that I can do something like this with Python tuples. It just does not have the vibe of the tuple’s immutability. The way I see it, if you have an immutable type, you should be able to copy it, you should be able to concatenate two instances, and the like – but youshouldnotbe able to assign a new tuple to an old name using an in-place operation. You want this name to be the same? Your choice. So, I am fine with this: as it means assigningx + ytox, which basically means overwriting this name. If you choose to overwrite the previous value ofx, it’s your choice. But in-place operations, at least in my eyes, do not have the feeling of immutability. I’d prefer tonotbe able to do this in Python. If not immutability, then maybe something else should convince me to use tuples more often? But what? Performance? Tuples’ performance is poor, so this does not convince me. In terms of execution time, there is no discussion; they are definitely slower than the corresponding lists. You may say that in terms of memory. Indeed, they do take less disk space, but the difference is subtle, and for long containers – totally negligible. RAM memory use? This argument also turned out to not be too successful, because generally, lists turned out to be as efficient as tuples – and sometimes even more efficient. And if we have a huge collection, a generator will do better in terms of memory. Despite all that, tuples do have their place in Python. They are very frequently used to return two or three items from a function or method – so, as small unnamed records. They are used as the output of iterable unpacking. And they constitute the base of named tuples –collections.namedtupleandtyping.NamedTuple– the tuple’s powerful siblings that can be used as records with named fields. All in all, I do not like tuples as much as I did before writing this article. I treated them as an important Python type; now it’s not as important in my eyes as it was – but I accept their various use cases in Python, and I even like some of them. Am I unfair for tuples? Maybe. If you think so, please let me know in the comments. I always enjoy fruitful discussions with my readers. Thank you for reading. If you enjoyed this article, you may also enjoy other articles I wrote; you will see themhere. If you want to join Medium, please use my referral link below: Join Medium with my referral link – Marcin Kozak A Guide to Python Comprehensions PEP 3132 – Extended Iterable Unpacking Fluent Python, 2nd Edition In this Appendix, you will find the script I used for benchmarking tuples against lists. I used theperftesterpackage, about which you can read in this article: Benchmarking Python Functions the Easy Way: perftester This is the code: And here are the results: I decided to run memory-usage benchmarks for much biggern, that is, of 5 and 10 million. I will not present the code here, and if you have some time to spare, you may consider it a nice exercise to write it, based on the above script. If you just want to see the code, however, you will find ithere. Note that I could make the code better, as I could join the code for the two experiments. I decided not to do that, in order to keep the two scripts rather simple. Here are the results: As you see, for the operations we study, tuples take either the same or more memory – sometimes even significantly more (compare, for instance,554.8vs478.5or783.7vs707.4). Written By  Share This Article Towards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program. Step-by-step code guide to building a Convolutional Neural Network Here’s how to use Autoencoders to detect signals with anomalies in a few lines of… Solving the resource constrained project scheduling problem (RCPSP) with D-Wave’s hybrid constrained quadratic model (CQM) An illustrated guide on essential machine learning concepts Derivation and practical examples of this powerful concept Columns on TDS are carefully curated collections of posts on a particular idea or category… With demos, our new solution, and a video Your home for data science and Al. The world’s leading publication for
				data science, data analytics, data engineering, machine learning, and
				artificial intelligence professionals. Is the AI and Data Job Market Dead? | Towards Data Science Detecting and Editing Visual Objects with Gemini | Towards Data Science How to Personalize Claude Code | Towards Data Science How to Run Claude Code for Free with Local and Cloud Models from Ollama | Towards Data Science All articles on Towards Data Science are available to registered users. Create a free account today to access this article and thousands more."
4,Going Under the Hood of Character-Level RNNs: A NumPy-based Implementation Guide,"Publish AI, ML & data-science insights to a global community of data professionals. Due to the recent boom of LLMs, it is imperative to grasp the rudiments of language modeling Recurrent neural networks (RNNs) are a powerful type of neural network that have the ability to process sequential data, such as time series or natural language. In this article, we will walk through the process of building a Vanilla RNN from scratch using NumPy. We will begin by discussing the theory and intuition behind RNNs, including their architecture and the types of problems they are well-suited for solving. Next, we will dive into the code, explaining the various components of the RNN and how they interact with one another. Finally, we will demonstrate the effectiveness of our RNN by applying it to a real-world dataset. Specifically, we will be implementing a many-to-many, character-level RNN that uses sequential, online learning. This means that the network processes the input sequences one character at a time and updates the parameters of the network after each character. This allows the network to learn on-the-fly and adapt to new patterns in the data as they are encountered. A character-level RNN means that the input and output are individual characters, rather than words or sentences. This allows the network to learn the underlying patterns and dependencies between characters in a piece of text. The many-to-many architecture refers to the fact that the network receives an input sequence of characters and generates an output sequence of characters. This is different from a many-to-one architecture, where the network receives an input sequence and generates only one output, or a one-to-many architecture, where the network receives only one input and generates an output sequence. I used Andrej Karpathy’s code (found[here](https://github.com/j0sephsasson/numpy-NN)) as a foundation for my implementation, making several modifications to improve versatility and reliability. I expanded the code to support multiple layers, and also restructured it for better readability and reusability. This project builds on my previous work of creating a simple ANN using NumPy. The source code for that can be found here. RNNs can be contrasted with traditional feedforward neural networks (ANNs), which do not have a ""memory"" mechanism and process each input independently. ANNs are well-suited for problems where the input and output have a fixed size and the input does not contain sequential dependencies. In contrast, RNNs are able to handle variable length input sequences and maintain a ""memory"" of past inputs through ahidden state. The hidden state allows RNNs to capture temporal dependencies and make predictions based on the entire input sequence. To summarize, the network uses information from previous time steps to inform its processing of current inputs. Additionally, more complex NLP architectures can handle long-term dependencies (GPT-3 was trained using a sequence length of 2048), where information from the beginning of the input sequence is still relevant for predicting the output at the end of the sequence. This ability to maintain a ""memory"" gives RNNs & transformers a significant advantage over ANNs when it comes to processing sequential data. Recently, transformer architectures such asGPT-3andBERThave become increasingly popular for a wide range of NLP tasks. These architectures are based onself-attentionmechanisms that allow the network to selectively focus on different parts of the input sequence. This allows the network to capture long-term dependencies without the need for recurrence, making it more efficient and easier to train than RNNs. The transformer architectures have been shown to achieve state-of-the-art results on a wide range of NLP tasks and have been used in many real-world applications. Although the transformer architectures are more complex than the vanilla RNNs and have different characteristics, the vanilla RNNs still have an important role to play in the field of deep learning. They are simple to understand, easy to implement and debug, and can be used as a building block for other more complex architectures. In this article, we will focus on the vanilla RNNs and peek under the hood to see how they really work. We will be implementing the many-to-many architecture as seen below. From this point on, we will denote the hidden state at time step t usingh[t]. In the figure this iss[t]. As you can see, the hidden state from the previous time steph[t-1]is combined with the current inputx[t], and this is repeated over the number of time steps. Inside the RNN block, we are updating the hidden state for the current time step. For clarification, a time step is just a character, such as ‘a’ or ‘d’. An input sequence contains a variable number of characters or time steps, also known as sequence length, which is a hyper-parameter of the network. We are reading in the data as a string, from a plain text file, and tokenizing the characters. Each unique character (there are 65), will be mapped to an integer and vice-versa. Let’s sample an input & target sequence to our RNN. The inputs are a tokenized sequence, the targets are the inputs offset by one. Lets start by discussing the components of an RNN, in contrast to a basic ANN. In a traditional feedforward neural network, the parameters governing the interactions between layers are represented by a single weight matrix, denoted asW. However, in a Recurrent Neural Network (RNN), the interactions between layers are represented by multiple matrices. In my code, these matrices are specifically:Wxh,Whh, andWhy, representing the weights between input and hidden layers, hidden to hidden layers, and hidden to output layers respectively. TheWxhmatrix connects the input layer to the hidden layer, and is used to transform the input at each time step into a set of activations for the hidden layer. TheWhhmatrix connects the hidden layer at time step t-1 to the hidden layer at time step t, and is used to propagate the hidden state from one time step to the next. TheWhymatrix connects the hidden layer to the output layer, and is used to transform the hidden state into the final output of the network. In summary, the main difference between the weights in an ANN and an RNN is that the ANN has one weight matrix, while the RNN has multiple weight matrices that are used to transform the input, propagate the hidden state, and produce the final output. These multiple weights matrices in the RNN allow it to maintain a memory of past inputs and move information through time. The Constructor Here we are defining our RNN parameters as discussed above. Something interesting to take note of – the parametersWhyandbyrepresent a linear layer, and could be abstracted even more, into a separate class such as PyTorch’s ‘nn.Linear’ module. However, we will keep them as a part of the RNN class for this implementation. Let’s start at the top, and break it down. What is happening here? Outside the loop, once per sequence Inside the loop, for every time step in the sequence Dropout is aregularizationtechnique that aims to prevent overfitting by randomly ""dropping out"" (setting to zero) certain neurons during the training process. In the code above, the dropout layer is applied before updating the hidden state at time step t, layer l, by multiplying the hidden state at t-1 with a dropout mask. The dropout mask is generated by creating a binary mask where each element is 1 with probability p and 0 otherwise. By doing this, we are randomly ""dropping out"" a certain number of neurons in the hidden state, which helps to prevent the network from becoming too dependent on any single neuron. This makes the network more robust and less likely to overfit on the training data. After applying dropout, the hidden state is scaled back by dividing it by (1-p) to ensure that the expected value of the hidden state is maintained. Calculations forys[t]andps[t]are outside the second loop because there is only one linear layer, as opposed to an arbitrary number of RNN layers. The choice was made to use the indexing[t][l]to store the hidden state for the l-th layer at time step t. This is because the model processes the input sequence one timestep at a time, and at each timestep, it updates the hidden state for each layer. By using the indexing[t][l], we are able to keep track of the hidden state for each layer at each timestep, allowing us to easily perform the necessary computations for the forward pass. Additionally, this indexing allows for easy access to the hidden state of the last timestep, which is returned ashs[len(x)-1], as it is the hidden state of the last timestep in the sequence for each layer. This returned hidden state is used as the initial hidden state for the next sequence during the training process. Let’s perform the forward pass. Remember, there is no batch dimension. First, some intuition for the backwards pass of a RNN. The key difference between backpropagation in a basic ANN and an RNN is the way the error is propagated through the network. While both ANNs and RNNs propagate the error from the output layer to the input layer, RNNs also propagate the error backwards through time, adjusting the weights and biases at each time step. This allows RNNs to process sequential data and maintain a ""memory"" in the form of its hidden state. The BPTT (backpropagation through time) algorithm works by unrolling the RNN over time, creating a computational graph for each time step. The graph for this network can be seenhere. The gradients are then calculated for each time step and accumulated over the entire sequence. Same as the forward pass, let’s break it down. The first thing this function does is initialize the gradients for the weights and biases to zero, similar to what happens in a feedforward ANN. This is something that confused me, so I am going to elaborate a bit more. By resetting the gradients to zero before every sequence, it ensures that the gradients calculated for the current sequence do not accumulate or add up with the gradients calculated in the previous sequences. This prevents the gradients from becoming too large, which can cause the optimization process to diverge and negatively impact model performance. Additionally, it allows for weight updates to be performed independently for each sequence, which can lead to more stable and consistent optimization. Then, it loops through the input sequence in reverse, performing the following computations for each time step t: Notice the comment, backprop into y, that link will explain what is happening perfectly. I also go into depth on this in a previous article you can check outhere. What is the difference between dh and dhraw?Good question. The difference betweendhanddhrawis thatdhis the gradient of the hidden statehs[t][l]with respect to the loss, computed by backpropagating the gradient of the probabilitiesps[t]of the softmax activation of the output layer.dhrawis the same gradient, but it is further backpropagated through the non-linear tanh activation function by element-wise multiplying the gradient of the hidden statedhwith the derivative of the tanh function, which is (1 –hs[t][l]*hs[t][l]). Let’s perform the backward pass. Finally, we return the gradients so we can update the parameters, and that is a segway into my next topic – the optimization. Like it says in the RNN class ‘update’ method, we will be using Adagrad for this implementation. Adagrad is an optimization algorithm that adapts the learning rate of each parameter in a neural network individually, based on the historical gradient information. It’s particularly useful for handling sparse data and is often used in natural language processing tasks. Adagrad makes adjustments to the learning rate at each iteration, ensuring that the model learns from the data as quickly and efficiently as possible. This block of code updates the parameters of the RNN using the Adagrad optimization algorithm. It keeps track of the sum of squares of the gradients of the parameters (mWxh,mWhh,mbh,mWhyandmby) and divides the learning rate with the square root of that sum plus a small constant value of1e-8, to ensure numerical stability, effectively adjusting the learning rate for each parameter. Additionally, it clips the gradients to preventexploding gradients. Adagrad adapts the learning rate for each parameter, performing larger updates for infrequent parameters and smaller updates for frequent parameters. Meaning, parameters that are updated infrequently, the learning rate will be larger, so that the model can make bigger adjustments to those parameters. On the other hand, for parameters that are updated frequently, the learning rate will be smaller, so that the model can make small adjustments to those parameters, preventing overfitting. This is in contrast to using a fixed learning rate, which could either under-correct or over-correct the parameters. Let’s perform the parameter update. The final piece is actually training the network, where the input sequences are fed through the network, the error is calculated and the optimizer updates the weights and biases. This block of code is pretty straightforward. We are performing a forward and backward pass, and updating the model parameters every epoch. Something I would like to point out – The loss is updated by a weighted average of the current loss and the previous loss. The current loss is multiplied by 0.001 and added to the previous loss, which is multiplied by 0.999. This means that the current loss will only have a small impact on the total loss, while the previous losses will have a larger impact. This way, the total loss will not fluctuate as much, and will be more stable over time. By using an EMA (exponential moving average), it is easier to monitor the performance of the network and detect when it is overfitting or underfitting. The training process of our RNN has been successful, we can see the decrease in loss and the improved quality of the generated samples. However, it is important to note that generating original Shakespeare is a complex task, and this particular implementation is a simple vanilla RNN. Therefore, there is room for further improvement and experimentation with different architectures and techniques. In conclusion, this article has demonstrated the implementation and training of a character-level RNN using Numpy. The many-to-many architecture and online learning approach allows the network to adapt to new patterns in the data as they are encountered, resulting in improved sample generation. While this network is quasi-capable of generating original Shakespeare text, it is important to note that this is a simplified version and there are many other architectures and techniques that can be explored for much better performance. Full code & repohere. Feel free to get in touch & ask questions, or make improvements to the code. Thanks for reading! Written By  Share This Article Towards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program. Step-by-step code guide to building a Convolutional Neural Network A deep dive on the meaning of understanding and how it applies to LLMs A beginner’s guide to forecast reconciliation Here’s how to use Autoencoders to detect signals with anomalies in a few lines of… Feature engineering, structuring unstructured data, and lead scoring Solving the resource constrained project scheduling problem (RCPSP) with D-Wave’s hybrid constrained quadratic model (CQM) An illustrated guide on essential machine learning concepts Your home for data science and Al. The world’s leading publication for
				data science, data analytics, data engineering, machine learning, and
				artificial intelligence professionals. Is the AI and Data Job Market Dead? | Towards Data Science Detecting and Editing Visual Objects with Gemini | Towards Data Science How to Personalize Claude Code | Towards Data Science How to Run Claude Code for Free with Local and Cloud Models from Ollama | Towards Data Science All articles on Towards Data Science are available to registered users. Create a free account today to access this article and thousands more."
5,10 Subtle Strategies I Wish I Knew When I Had 23 Email Subscribers and Made $0 Online,"There are people who blow up online in a year. My friend Dakota is one of them. From no audience in 2021 to $70,000 in a month in 2022/2023. It’s easy to brush off these stories and think they’re impossible. What’s odd is when you know these people before and after their achievements, you realize they’re not that smart. They’ve just taken the time to understand a few basic rules of the online game. You can too. Here are the patterns of those online who get off $0. I spent more years than I’d like to admitearning $0 online. I got crucified for it by my friends and family. “You’re wasting your time. Go get a real job ya bum.” Looking back, one of the issues was I never looked at data. I just came up with a random idea. Then I published that idea on any of the many platforms available. It’s like walking into a room full of aircraft pilots and randomly talking about your love for bees and honey. There may be a beekeeper amongst…"
6,How To Start A Niche Site in Under 3 Hours (With Checklist),"You don’t have time for a side hustle. Right? Bullshit. For the next year, I’m going to be building a profitable niche site in one hour a day. Because you don’t need all the time in the world. You just need to start. Today I’m sharing stage 1. How I went from zero ideas to a published website ready for content in under three hours. My first blog failed because it was too general and the advice was shit. I worked on it for three months and backed out for something more commercial. But I began learning how to write online. My second niche site failed. It was about extended-range guitars but playing guitar is going out of fashion faster than riding to work on a hoverboard. I worked on it for about 4 months, earned a little affiliate commission and realised it wasn’t a viable niche. But I learned about affiliate marketing and SEO. My third site brought my first 6 months of learning together and hit the target. I carried on learning and worked on it for the next four years before selling it."
7,Don’t Become a Full-Time Content Creator If You Have Low-Risk Tolerance,"Making the leap to full-time content creation can be an exciting and rewarding career choice, but if you have low-risk tolerance, it may not be the right move for you. Trust me, it’s full of risks, unpredictability, and uncertainty. In this article, we’ll discuss why it’s important to consider your risk tolerance when deciding to become a full-time content creator. I quit my full-time job without savings. I Ubered and Lyfted to supplement my blogging income while scrambling to get clients and contracts and built a base income on multiple platforms. I’ve lost 20K pageviews overnight on my portfolio of websites after an endless stream of Google algorithm updates. There is a ton of risk in being a solopreneur. My risk tolerance is pretty high. If not, I would have given up 100 times. Getting into the blogging game is not for the faint of heart, so please be warned that it’s not all remote vacations, backpacking through Europe, and sipping champagne on a balcony in Paris. As an aspiring content creator, you may be tempted to take the plunge and go full-time as soon as they start to gain traction and become successful. But before taking such a big move, it’s important that aspiring creators assess their risk tolerance levels and understand the possible consequences of such a leap. Going full-time requires a substantial commitment of time, energy, and resources. And while there are certainly rewards in terms of creativity and access to more opportunities, there are also real risks associated with giving up steady income or relying solely on uncertain outcomes like ad revenue or product sales. Before jumping into content creation full-time, aspiring creators should consider the following factors: Take the time upfront to assess your risk tolerance and do some soul-searching. If nothing else, you will be more prepared than ever to take the leap. Aspiring creators often forget about the emotional risks. Yet, going full-time can be both an exhilarating and exhausting experience — one that requires resilience, copious amounts of self-belief, and an almost delusionally thick skin. Just earlier today I’ve been called: As a content creator, it’s not uncommon to experience moments of euphoria when your hard work pays off and you receive recognition in the form of likes, shares, or new subscribers. However, this can quickly be followed by steep drops into despair. You might feel disappointed in yourself, self-doubt, rejection, failure, and hopeless. It’s important to remember that these emotions are all part of the journey. Content creation is essentially a rollercoaster ride — and being prepared for the highs and lows on this journey is key for anyone looking to go full-time in the industry. Even with all of this risk, I wouldn’t change my job or my decision to go full-time in 2020. This is by far the best job I’ve ever had. Thanks for reading! Related posts: Don’t miss my next article — sign up for myMedium email list christopherkokoski.medium.com"
8,Why Storytelling Is A Critical Skill For Startup Founders,"Aspiring to become a startup founder? Learn to weave words into stories to give people a reason to trust your business idea of changing the world. “Long before the first business was established, the most powerful words in any language were “Let me tell you a story.” — MATHEWS & WACKER, What’s Your Story I love listening to stories from successful founders about their startup struggles and how they achieved exceptional growth. And what I have observed so far is that the truly exceptional pitches don’t just focus on unmet needs and market potential, but also include a combination of data, facts, and strategy, all woven together with a personal story, shaping the founder’s perspective. It’s the best I could say,“business storytelling in its finest form.” We’ve all been brought up listening to stories from our moms, grandmas, teachers, and leaders. However, it’s not just me who loves stories; the human species are destined to think in metaphors and learn through stories. Stories have the innate power to inspire, move decisions, and leave a deep impression. When a startup is formed, there are only three things: a great idea, an immense passion, and a purpose. But none of this may be understood by…"
9,Why My Side Hustle Stuck After 3 Years and What I Want to Do to Improve It,"If you are not subscribed as a Medium Member, please consider subscribing throughmy referral. I always have believed that we need to diversify our income. Never put all our eggs in one basket. That is why I tried my best these past three years to build my side hustles incomes. The problem is what I have imagined needs to be aligned with reality.My side hustles grow less than I want.I was stuck in the hole that I built myself. Nevertheless, it’s a lesson I take from all this time trying to build something new. But, if I could turn back time, there are things I wish to fix as I already identified the core problems. Although, it’s never too late to try to improve something. So, here are some reasons why my side hustles were stuck and what I want to do to improve them. There are many things that I could think of as mistakes, but what I pinpoint as the significant issues were:"
10,Kill Your Perfectionism with an Abundance Mindset,"You know what’s a scam? Something millions of people blow thousands of dollars on every year? Something that’s considered rare and an expression of love and thus worth a crap ton of money? Diamonds. But diamonds aren’t rare at all. We only think they are because ofartificial scarcity. And that’s why we treat them as precious. Likewise, we perfectionists fall into the same trap. We believe opportunities to do good work are rare. And thus we treat each piece of writing as precious. We forget that opportunities to write new — and better!— stories are as common as sand in a walrus’s butt crack. And thus we approach writing like a perfectionist marathon runner: we don’t make it far because we keep going back and forth over and over trying to get our very first step juuust right. Imagine you’re a spacefaring chef. Instead of writing words, you cook food. But unlike an ordinary chef, you’re not looking to open a restaurant here on planet Earth. Instead, you’ve decided to open one on planet BeebleBoff where there are lots of potential customers."
11,"For this Writer, Self-Promotion isn’t Easy","When my first general non-fiction book came out in 2019, I had no practical experience of promoting a book. My fantasy about the process was fuelled by countless film scenes depicting newly published authors, and it went something like this. I would have a dedicated public-relations person, perhaps even a PR team, whipped along by my hard-nosed and loyal literary agent. This agent and my publisher would arrange everything, including all those readings to rapt crowds of people, sitting on folding chairs in nice bookstores and on the risers at well-respected literary festivals. I’d be collected by minders at airports during my whirlwind tours, like the essayist David Sedaris describes in his latest diaries installment,The Carnival of Snackery. I’m no Sedaris, but I did assume that as atraditionally published author, everything would be taken care of for me. That would have been a good thing, because while I might be able to pen a whole book, putting together a two-line bio on Twitter or LinkedIn sends me into paroxysms of writer’s block. Luckily, I was assigned an experienced PR person at my last publishing house.Luxury!I thought.I’ll place myself in her capable…"
12,How to Start Your Novel with Momentum to Finish It,"There are no magic formulas to starting and finishing your novel. Even novelist who have written several books often suffer with starting a new one. The fear of the blank page is something that stares at all writers. Even highly successful novelist struggle with starting a new book. From the Cambridge University Presswebsite, Ernest Hemingway is noted as saying, “There is nothing to writing. All you do is sit down at a typewriter and bleed.” Hemingway, a Pulitzer Prize Winner, is saying that writing is difficult. As a writer you have to deal with negative voices, lack of confidence, and the many pains of telling a story well. There are so many reasons not to start writing. Do I really need to list them? My computer is another room. I have reports for work that I need to finish first. My daughter needs help with her math. I haven’t made a home cooked meal in a week. I am not a great writer. I will never finish this goal. And yet, here you are reading this article. You want to start your novel. There has been this gnawing in you and what better time than now."
13,Coupon Collector’s Problem: A Probability Masterpiece,"Publish AI, ML & data-science insights to a global community of data professionals. Unpacking the intricacies of a classic probability puzzle The world of mathematics is full of fascinating puzzles and paradoxes that challenge our understanding of the world around us. The coupon collector problem is one such puzzle, an age-old problem that has intrigued mathematicians and statisticians for generations. The coupon collector problem is as follows: Consider a contest involving collecting n unique coupons. It is given that every cereal box, contains one coupon, with every coupon being equally likely to be present in a given box. How many boxes do you need to open before you have at least one coupon of each type? This seemingly simple question has a surprisingly complex answer that has applications in fields ranging from computer science to finance. In this article, we will delve into the Coupon Collector’s Problem, explore its framework, and discuss its many fascinating implications. Before solving this problem, let’s revisit some of the properties of the geometric random variable. This will provide an efficient mechanism to compute various parameters describing the architecture of the solution to the problem. Definition: A random variable X is said to follow a geometric distribution with parameter p if its probability mass function is given as follows: Intuitively, a geometric random variable gives the distribution of the number of independent trials required to achieve success, under the assumption that the probability of success remains constant i.e., p. So, finding P(X = k) is equivalent to finding the probability that k trials are required to achieve a success i.e., there are k – 1 failures before the first success. The required probability is thus: Geometric random variables have many applications in real-world problems, such as modeling the number of attempts needed to win a game or the number of calls required to reach a customer at a call center. The Coupon Collectors problem can also be modeled effectively using geometric random variables (specifically a sum of independent, nonidentically distributed geometric variables, which we’ll be seeing later). A geometric random variable has the following properties: Optional Proof:First, we will determine the expected value of a geometric random variable. Recall that the expected value of a random variable is like a weighted average of its values, with the weights given by its probability mass function. It can be shown that the expected value of a geometric random variable is the reciprocal of its parameter p i.e., the probability of success. The proof is as follows: To evaluate the above expression, we use a small trick. It is known that p is a continuous parameter with values bounded between 0 and 1 (as it’s a measure of probability). Finding the sum of an infinite series can become very easy if we can model it as a geometric series with a constant common ratio r. Then, We need to find a way to convert the given infinite series sum into a geometric series that can be easily evaluated using the above expression. To do so, we recall that the derivative of x^i is i*x^(i−1), something very similar to the form above, provided we replace x with 1 – p. Thus, we have: We can take the derivative outside the sum as the sum of derivatives is equal to the derivative of the sum. Thus, We may now use the infinite geometric series sum to obtain: The above result is standard in most probability textbooks. For this article, understanding how to use the above result is more important than understanding how the result is derived. Thus, we’ve shown that the expected value of a geometric random variable is the reciprocal of its parameter p. Now, we proceed to find the variance of the geometric random variable. The math is slightly complicated, you may choose to skip over it if you’re simply interested in solving the given problem. Calculating the variance of the geometric distribution: We use the following formula for variance: We now proceed to calculate E(X(X − 1)): Just as before, we observe that the second derivative of x^i with respect to x is i(i − 1)x^(i−2) Thus: Thus, Therefore, we have obtained the variance and expectation of the geometric distribution. To solve the problem at hand, we need to first model it, which involves identifying its parameters, assumptions, and relevant variables. VARIABLES: To find the solution to the problem, we need to determine the number of boxes that must be opened before we have at least one coupon of each type. This requires us to define appropriate variables that encapsulate the elements necessary for the solution. The nature and properties of these variables depend upon the field from which the problem derives its essence. Since the Coupon Collector’s Problem is rooted in the fields of statistics and probability, the nature of the variables will be stochastic or random. Thus, we define the random variable T to refer to the minimum number of boxes that need to be opened to collect all n unique coupons. We use a random variable for this problem because it is not guaranteed that T will take a certain value. In practice, T could be any value from n to infinity. The only questions we can answer are:
• What is the expected (or average) value of T?
• What is the variance (or average spread in the value) of T?
• What is the probability that more than c boxes need to be opened? In other words, what is the probability that T > c? After modeling the various elements of the problem, we have a clear understanding of its settings, scope, and expectation. We now proceed to work on its solution. Let’s start by analyzing the distribution of T. However, one will soon observe that the distribution of T is rather complex, and cannot be ascribed to any of the well-known probability distributions. This is because even though, by assumption, each coupon has an equal probability of being present in a given box, the probability of finding a new coupon decays as we collect more and more coupons. Consider the scenario where we are required to collect 100 unique coupons. When we open the first box, we are guaranteed to find a new coupon since we don’t have any coupons yet i.e., However, when we open the second box, the probability of finding a new coupon is slightly reduced as there is a chance (1/100) that we might find the same coupon that we found in the first box. As we continue opening more boxes, the probability of finding a new coupon keeps decreasing (not strictly) since there are fewer unique coupons left to collect. Thus, the presence of non-constant probability values makes it rather difficult to assign a common probability distribution to T. Thus, we resort to the method of divide and conquer, a very popular strategy to solve probability problems. This involves breaking down a problem into smaller chunks of reduced complexity and approaching each chunk independently. Since T measures the time or the number of boxes that need to be opened to collect n unique coupons, we break it down by proposing Tᵢ to measure the time or the number of boxes that need to be opened to collect the ith coupon. We can express T as a sum of all Ti : What’s the benefit? Well, the distribution of Tᵢ (unlike the distribution of T) is parameterized by a constant probability value. Let’s understand what this means. We can think of each box opening as a trial in a sequence of Bernoulli trials, where success means finding a new, previously unseen coupon. (Recall that Bernoulli trials are a type of random experiment with only two possible outcomes: success and failure. For example, flipping a coin is a Bernoulli trial, where success might be defined as getting heads, and 3 4 failure as getting tails.) At the start, we have not collected any coupons, so the probability of success on the first trial is n/n, or simply 1.0: On the second trial, there are n-1 unique coupons left to collect, out of a total of n coupons, so the probability of success is (n-1)/n. This is because there is only one coupon that would result in failure (i.e., a repeat of the coupon collected in the first trial), and n-1 coupons that would result in success (i.e., a new coupon that has not yet been collected). On the third trial, there are n-2 unique coupons left to collect, out of a total of n coupons, so the probability of success is (n-2)/n. Similarly, on the fourth trial, the probability of success is (n-3)/n, and so on. We can extrapolate the formula to find the probability of collecting the ith unique coupon (i.e., the probability of finding a unique coupon on opening a box given that we have already collected i – 1 unique coupons): Recall that Tᵢ measures the number of independent trials to collect the ith unique coupon. This sounds familiar; yes it’s the geometric random variable! Each of these trials corresponds to a Bernoulli trial where success means finding a new, previously unseen coupon given that i – 1 unique coupons have been collected. Thus, Therefore, we can see T as a sum of non-identical geometric distributions: We can now proceed to answer the questions proposed earlier. To find the expected value of T, we use the property that the expected value of a sum of random variables is the sum of their expected values: Since Tᵢ is a geometric random variable: Thus, where H(n) refers to the nth harmonic number: It can be asymptotically approximated (for large values of n) as: where γ ≈ 0.577215665 is the Euler–Mascheroni constant. We can think of it as an estimate of the average number of boxes one would need to open to collect all n coupons if one repeated the coupon collection process many times. For example, suppose you wanted to collect all 100 unique coupons from a set of promotional cereal boxes, and you plan to buy one box at a time until you have collected them all. The formula n * H(n) would estimate the average number of boxes you would need to buy to complete your collection, assuming that each box has an equal chance of containing any of the 100 coupons. The following python code allows us to calculate this value: Of course, in reality, the actual number of boxes you would need to buy to collect all 100 coupons would vary from trial to trial, depending on the specific coupons you get in each box. However, the formula n * H(n) gives us an idea of what to expect on average, and it can be a useful tool for planning and budgeting. For larger values of n, the formula predicts that more boxes will need to be opened to collect all n unique coupons. This is because the probability of finding a new coupon decreases as the number of coupons already collected increases. The harmonic number H(n) grows logarithmically with n, so the expected number of boxes grows roughly proportional to n ln n. This implies that collecting a large number of unique coupons can be a challenging and time-consuming task, which matches our intuition. Next, we try to calculate the variance of T to have an idea of how the number of boxes we need to collect varies from trial to trial. Since all trials are independent (by the assumption that each coupon has an equal probability of being present in a given box), the random variables Tᵢ, Tⱼ; i != j are also independent. Thus, the variance of their sum is the sum of their variances. Thus, Since Tᵢ is a geometric random variable: Thus, Where we have used Euler’s approach to the Basel problem: For example, suppose you wanted to collect all 100 unique coupons, and you repeated the process many times. The variance would give you an estimate of how much the number of boxes required varies from trial to trial. If the variance is small, then you can expect to need a similar number of boxes in each trial. If the variance is large, then the number of boxes required could vary widely from trial to trial. The variance is proportional to n^2, so for larger values of n, the variance grows much faster than the expected number of boxes required. This implies that as n gets larger, the number of boxes required to collect all n coupons becomes more and more unpredictable, and it becomes increasingly difficult to estimate how many boxes you will need to buy on average. Finally, we try to calculate (or at least bound) the probability that the number of boxes that need to be opened to collect all n unique coupons exceeds (or equals) c i.e., the probability that T takes a value more than or equal to c. Since the distribution of T is rather complex, it’s difficult to find the exact value of such a probability. However, statistics provides us with many useful inequalities that can help us bound the value of the probability. Specifically, we shall use the Markov inequality to upper bound the probability that T takes a value more than or equal to c. Markov’s inequality is a powerful tool in probability theory that allows us to make general statements about the probability of a random variable exceeding a certain value. The inequality is named after the Russian mathematician Andrey Markov, who first introduced it in the late 19th century. The Markov inequality states that for any non-negative random variable X and any positive number a, the probability that X is greater than or equal to a is less than or equal to the expected value of X divided by a. In mathematical notation, this can be written as: Intuitively, the Markov inequality says that if we want to know how likely it is for a random variable to take on a value greater than or equal to a, we can bound this probability by dividing the expected value of the random variable by a. This bound is often a very loose one, but it can be useful in situations where we have limited information about the distribution of the random variable. Since T is non-negative (the number of boxes cannot be negative), we can use the Markov inequality: The above approximation can be useful for estimating the probability when the value of n is very large. For example, suppose we want to estimate the probability that we need to open more than 1000 boxes to collect all 100 unique coupons. We can use the inequality to obtain an upper bound on this probability as: Therefore, if we know the values of n and c, we can use this bound to estimate the probability that we need to open more than c boxes to collect all n unique coupons. In conclusion, the coupon collector’s problem is a classic problem in probability theory that has a wide range of practical applications. The problem involves collecting a set of N unique coupons, where each coupon is equally likely to be found in a given box. We have discussed various aspects of this problem, including the expected value and variance of the time it takes to collect all n unique coupons as well as the probability that more than a given number of boxes are needed to collect all n unique coupons. The coupon collector’s problem is a fascinating problem with many interesting properties and applications. It is an important tool for understanding probability and statistics, and its solutions have a wide range of real-world applications, from designing surveys and collecting data to analyzing customer behavior and predicting market trends. By understanding the coupon collector’s problem, we can gain valuable insights into the behavior of complex systems and make more informed decisions in a variety of contexts. Thanks for reading! Hope you enjoyed this article! Written By  Share This Article Towards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program. Step-by-step code guide to building a Convolutional Neural Network A beginner’s guide to forecast reconciliation Here’s how to use Autoencoders to detect signals with anomalies in a few lines of… Solving the resource constrained project scheduling problem (RCPSP) with D-Wave’s hybrid constrained quadratic model (CQM) An illustrated guide on essential machine learning concepts Derivation and practical examples of this powerful concept Columns on TDS are carefully curated collections of posts on a particular idea or category… Your home for data science and Al. The world’s leading publication for
			data science, data analytics, data engineering, machine learning, and
			artificial intelligence professionals. Is the AI and Data Job Market Dead? | Towards Data Science How to Personalize Claude Code | Towards Data Science Why Your ML Model Works in Training But Fails in Production | Towards Data Science How to Run Claude Code for Free with Local and Cloud Models from Ollama | Towards Data Science All articles on Towards Data Science are available to registered users. Create a free account today to access this article and thousands more."
14,Sparkles aren’t good UX,"I will be the first person to tell you how much I love the sparkle emoji. ✨ I use them constantly, and in most personal projects, Ithrow them onto my designslike glitter. They give things a little more glitz, a little more glam, and the four-pointed star cluster is very much in vogue at the moment. Its meaning is almost always positive, so why shouldn’t UX designers use sparkles in their designs? The four-pointed sparkle motif can be traced to the (becoming popular once more) 1950’s aesthetic. The atomic eraslapped stars and boomerang blobson wallpaper, advertisements, and vinyl diner booths. It’s a design that communicates something beautiful and new, and the stars’ proliferation during this decade perhaps reflects the optimism for new leaps forward in technology at the time. Just look atThe Jetsons: this futuristic cartoon family was animated in the early ’60s but was set in the far-off future of 2062. In the cartoon, wherein science fiction borders on outright magic…"
